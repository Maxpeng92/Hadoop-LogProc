{"name":"Hadoop Logproc","tagline":"","body":"LogProc\r\n=======\r\nIn this project, I implemented different distributed join algorithms which are described in: \r\n\r\n**\"A Comparison of Join Algorithms for Log Processing in MapReduce\"** of *Spyros Blanas, Jignesh M. Patel, Vuk Ercegovac, Jun Rao, Eugene J. Shekita and Yuanyuan Tian*\r\n\r\nYou can find the paper online at http://www.cs.ucr.edu/~tsotras/cs260/F12/LogProc.pdf, or in [material](/material) directory of this project.\r\n\r\n**Abbreviations**: L is log table and R is reference table\r\n\r\nStandard Repartition Join\r\n=======\r\nIt is also the join algorithm provided in the contributed join package of Hadoop **(org.apache.hadoop.contrib. utils.join)**. L and R are dynamically partitioned on the join key and the corresponding pairs of partitions are joined. \r\n\r\n\tMap phase: \r\n\t\t- Each map task works on a split of either R or L, uses tagging to identify with is original table\r\n\t\t- Input: (K: null, V : a record from a split of either R or L)\r\n\t\t- Output: (join key, tagged record)\r\n\r\n\tReduce phase:\r\n\t\t- Input: (K': join key, LIST V': records from R and L with join key K')\r\n\t\t- Buffer records from R (Br) and buffer records from L (Bl)\r\n\t\t- Performs a cross-product between records in set of Br and Bl.\r\n\t\t- Output: (null, r*l) which r*l is joined record\r\n\r\n**Potential problem:** When the key cardinality is small or when the data is highly skewed, all the records for a given join key may not fit in memory (Br + Bl, mostly Bl), R and L are moved across the network.\r\n\r\nImproved Repartition Join\r\n=======\r\nTo fix the buffering problem of the standard repartition join which is Bl too large.\r\n\r\n\tMap phase: \r\n\t\t- Each map task works on a split of either R or L, uses tagging to identify with is original table\r\n\t\t- Input: (K: null, V : a record from a split of either R or L)\r\n\t\t- Output: (composite_key, tagged record) with composite_key = (join key, tag)\r\n\r\n\tPartition phase: \r\n\t\t- Input: composite_key K\r\n\t\t- Output: partition of K which is hashcode(K.join_key) % num of reducers\r\n\r\n\tGrouping phase: \r\n\t\t- Input: composite_key K1 and K2\r\n\t\t- Output: group K1 and K2 if (K1.join_key == K2.join_key)\r\n\r\n\tSecondary sort phase:\r\n\t\t- Input: composite_key K1 and K2\r\n\t\t- Output: K1 < K2 (if K1.tag == ref && K2.tag == log)\r\n\r\n\tReduce phase:\r\n\t\t- Input: K′: a composite key with the join key and the tag, LIST V': records for K', first from R, then L\r\n\t\t- Buffer records from R (Br) - **only R**\r\n\t\t- Performs a cross-product between records in set of R and L.\r\n\t\t- Output: (null, r*l) with r*l is joined record\r\n\r\n**Potential problem:** Both versions (Standard Repartition Join and Improved Repartition Join) include two major sources of overhead that can hurt performance. In particular, both L and R have to be sorted and sent over the network during the shuffle phase of MapReduce.\r\n\r\nDirected Join\r\n=======\r\nThe shuffle overhead in the repartition join can be decreased if both L and R have already been partitioned on the join key before the join operation. This can be accomplished by pre-partitioning L on the join key as log records are generated and by pre-partitioning R on the join key when it is loaded into the DFS. Then at query time, matching partitions from L and R can be directly joined\r\n\t\r\n\tInits:\r\n\t\t- If Ri not exists in local storage then remotely retrieve Ri and store locally HRi ← build a hash table from Ri\r\n\r\n\tMap phase: \r\n\t\t- Input: (K: null, V: a record from a split of Li)\r\n\t\t- Output: join V with HRi by join key of V and join key of HRi\r\n\r\nWe can see directed join only store a hash table of Ri (part of R), which is small, so it is avoid run out of memory in the case of R is big or L table is skewed. The disadvantage of this approach is that R and L must be pre-partitioning.\r\n\r\nBroadcast Join\r\n=======\r\nUsually, R is much smaller than L. To avoid overhead due to storing and sending both tables, we can simply **broadcast** only R.\r\n\t\r\n\tInit phase:\r\n\t\t- if R not exist in local storage then\r\n\t\t\tremotely retrieve R\r\n\t\t\tpartition R into p chunks R1..Rp save R1..Rp to local storage\r\n\r\n\t\tif R < a split of L then\r\n\t\t\tHR ← build a hash table from R1..Rp\r\n\t\telse\r\n\t\t\tHL1 ..HLp ← initialize p hash tables for L\r\n\r\n\tMap phase: \r\n\t\t- Input: (K: null, V : a record from an L split)\r\n\t\t- Output: join V with HR or HLi (if HR is null)\r\n\r\nThe purpose of **init phase** is to hope that not all partitions of R have to be loaded in memory during the join. Besides that, to optimize the memory, the smaller of R and the split of L is chosen to build the hash table. \r\n\r\nNote that across map tasks, the partitions of R may be reloaded several times, since each map task runs as a separate process.\r\n\r\nSemi Join\r\n=======\r\nThis approach for the situation that R is large but many records in R may not be actually referenced by any records in table L. If we use **broadcast join**, there will be a large portion of the records in R that are shipped across the network (via the DFS) and loaded in the hash table are not used by the join. For a summary, we use semi Join to avoid sending the records in R over the network that will not join with L.\r\n\r\n\tPhase 1: Extract unique join keys in L to a single file L.uk (which is)\r\n\r\n\tPhase 2: Use L.uk to filter referenced R records; generate a file Ri for each R split\r\n\r\n\tPhase 3: Broadcast all Ri to each L split for the final join\r\n\r\nAlthough semi-join avoids sending the records in R over the network that will not join with L, it does this at the cost of an extra scan of L.\r\n\r\nPer-Split Semi-Join\r\n=======\r\nOne problem with semi-join is that not every record in the filtered version of R will join with a particular split Li of L. The per-split semi-join is designed to address this problem.\r\n\r\n\tPhase 1: Extract unique join keys for each L split to Li.uk\r\n\r\n\tPhase 2: Use Li.uk to filter referenced R; generate a file RLi for each Li\r\n\r\n\tPhase 3: Directed join between each RLi and Li pair\r\n\r\nCompared to the basic semi-join, the per-split semi-join makes the third phase even cheaper since it moves just the records in R that will join with each split of L. However, its first two phases are more involved.\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}